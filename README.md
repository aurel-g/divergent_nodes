# Divergent Nodes - Custom ComfyUI Nodes

This repository contains custom nodes for ComfyUI.

## Installation

1.  Clone this repository into your `ComfyUI/custom_nodes/` directory:
    ```bash
    cd ComfyUI/custom_nodes/
    git clone https://github.com/thedivergentai/divergent_nodes.git divergent_nodes
    ```
2.  **Set up API Key (for Gemini Node):**
    *   Create a `.env` file in the `divergent_nodes` directory (this directory).
    *   Add your Google AI Studio API key to the `.env` file like this: `GEMINI_API_KEY=YOUR_API_KEY_HERE`
    *   See the `.env.example` file for the format.
    *   The `.env` file is automatically ignored by git via `.gitignore`.
3.  Install/update the required Python dependencies:
    ```bash
    cd divergent_nodes
    pip install -r requirements.txt
    ```
4.  Restart ComfyUI.

The nodes should now be available in their respective categories when you right-click on the ComfyUI canvas.

## Nodes

### CLIP Token Counter

Counts the number of tokens generated by a CLIP tokenizer for the input text.

**Inputs:**

*   `text` (STRING): The text string you want to analyze.
*   `tokenizer_name` (STRING): The name of the Hugging Face CLIP tokenizer model to use (e.g., `openai/clip-vit-base-patch32`, `stabilityai/stable-diffusion-clip-vit-large-patch14`). Defaults to `openai/clip-vit-base-patch32`.

**Outputs:**

*   `token_count` (INT): The total number of tokens generated for the input text by the selected tokenizer.

**Category:** `Divergent Nodes ðŸ‘½/Text`

### Gemini API Node

Connects to the Google Gemini API to generate text based on a prompt and optional image input.

**Inputs:**

*   `model` (COMBO): Select the Gemini model to use. The list is dynamically fetched from the API if your API key is configured, otherwise defaults are shown.
*   `prompt` (STRING): The text prompt for the model.
*   `image_optional` (IMAGE): An optional image input for multimodal prompts.
*   `temperature` (FLOAT): Controls randomness (0.0-1.0). Lower values are more deterministic.
*   `top_p` (FLOAT): Nucleus sampling parameter (0.0-1.0).
*   `top_k` (INT): Top-k sampling parameter.
*   `max_output_tokens` (INT): Maximum number of tokens to generate.
*   `safety_harassment` (COMBO): Block threshold for harassment content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").
*   `safety_hate_speech` (COMBO): Block threshold for hate speech content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").
*   `safety_sexually_explicit` (COMBO): Block threshold for sexually explicit content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").
*   `safety_dangerous_content` (COMBO): Block threshold for dangerous content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").

**Outputs:**

*   `text` (STRING): The generated text response from the Gemini API.

**Category:** `Divergent Nodes ðŸ‘½/Gemini`

### KoboldCpp Node

Runs models using the KoboldCpp executable (`koboldcpp_cu12.exe` or similar). This node employs a hybrid approach: it launches a KoboldCpp instance in the background with specified setup parameters (and caches this instance for efficiency) and then sends generation requests to that instance's API.

**Prerequisites:**

*   You **must** have a working KoboldCpp executable (e.g., `koboldcpp_cu12.exe`). Download it from the [KoboldCpp releases page](https://github.com/LostRuins/koboldcpp/releases/latest).
*   The node needs the correct path to this executable (either the default path hardcoded in the script or provided via `koboldcpp_path`).
*   You need the `.gguf` model file(s) you intend to use.
*   For image input, you need the corresponding multimodal projector (`.gguf`) file specified in `mmproj_path`.
*   The `requests` Python library must be installed (`pip install -r requirements.txt`).

**How it Works (Hybrid Launch + API + Caching):**

1.  **Launch/Cache:** When executed, the node checks if a KoboldCpp instance with the *exact same setup parameters* (model, GPU layers, context size, etc.) is already running in its cache.
    *   **Cache Hit:** If a matching, responsive instance is found, it reuses that instance.
    *   **Cache Miss:** If no matching instance is running, it terminates any *other* cached instance (to conserve resources) and launches a *new* KoboldCpp process in the background using the current setup parameters. It finds a free port for the API and waits for the API to become ready. The new process is then added to the cache.
2.  **API Call:** The node prepares a JSON payload containing the generation parameters (prompt, image, temperature, max length, etc.).
3.  **Image Handling:** If an image is provided via `image_optional`, it's converted to a Base64 string and included in the JSON payload under the `"images"` key. The prompt text is automatically formatted to indicate an image is attached.
4.  **Request:** The JSON payload is sent via HTTP POST to the `/api/v1/generate` endpoint of the (cached or newly launched) KoboldCpp instance.
5.  **Response:** The node receives the JSON response from the API, extracts the generated text, and returns it.
6.  **Cleanup:** When ComfyUI exits normally, a cleanup function attempts to terminate all KoboldCpp processes launched and cached by this node. **Note:** If ComfyUI crashes or is force-killed, cached processes might remain running and require manual termination.

**Inputs:**

*   **Setup Arguments (Used for Launching/Caching):**
    *   `koboldcpp_path` (STRING): Full path to your `koboldcpp_cu12.exe` (or equivalent). Defaults to a common location but should be verified.
    *   `model_path` (STRING): Full path to the primary `.gguf` model file.
    *   `gpu_acceleration` (COMBO): Select the GPU backend ("None", "CuBLAS", "CLBlast", "Vulkan"). Defaults to "CuBLAS". Note: CLBlast defaults to platform/device 0 0; use `extra_cli_args` for others.
    *   `n_gpu_layers` (INT): Number of model layers to offload to the GPU (-1 for auto, 0 for CPU only). Defaults to -1.
    *   `context_size` (INT): Maximum context size for the model. Defaults to 4096.
    *   `mmproj_path` (STRING): Optional. Full path to the multimodal projector (`.gguf`) file if using a vision model. **Required for image input.**
    *   `threads` (INT): Number of CPU threads to use (0 for auto). Defaults to 0.
    *   `use_mmap` (BOOLEAN): Enable memory-mapped file loading. Defaults to True.
    *   `use_mlock` (BOOLEAN): Enable locking model in RAM. Defaults to False.
    *   `flash_attention` (BOOLEAN): Enable Flash Attention (requires compatible GPU/backend, usually CuBLAS). Defaults to False.
    *   `quant_kv` (COMBO): KV cache quantization level ("0: f16", "1: q8", "2: q4"). Defaults to "0: f16". Often requires Flash Attention.
    *   `extra_cli_args` (STRING): Optional. Add any other valid KoboldCpp *setup* command-line flags here (e.g., `--useclblast 1 0`, `--nommq`). **Do not** include generation flags like `--temp` or API-related flags like `--port`.
*   **Generation Arguments (Passed via API JSON):**
    *   `prompt` (STRING): The text prompt for the model.
    *   `max_length` (INT): Maximum number of tokens to generate. Defaults to 512.
    *   `temperature` (FLOAT): Controls randomness. Defaults to 0.7.
    *   `top_p` (FLOAT): Nucleus sampling parameter. Defaults to 0.92.
    *   `top_k` (INT): Top-k sampling parameter (0 to disable). Defaults to 0.
    *   `rep_pen` (FLOAT): Repetition penalty. Defaults to 1.1.
    *   `image_optional` (IMAGE): Optional image input. If provided, it's converted to Base64 and sent in the JSON payload. Requires `mmproj_path` to be set correctly.
    *   `stop_sequence` (STRING): Optional. Comma or newline-separated list of sequences to stop generation at.

**Outputs:**

*   `text` (STRING): The generated text response from KoboldCpp. Errors during execution will also be returned in this string.

**Category:** `Divergent Nodes ðŸ‘½/KoboldCpp`
