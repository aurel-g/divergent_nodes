# Divergent Nodes - Custom ComfyUI Nodes

This repository contains custom nodes for ComfyUI.

## Installation

1.  Clone this repository into your `ComfyUI/custom_nodes/` directory:
    ```bash
    cd ComfyUI/custom_nodes/
    git clone https://github.com/thedivergentai/divergent_nodes.git divergent_nodes
    ```
2.  **Set up API Key (for Gemini Node):**
    *   Create a `.env` file in the `divergent_nodes` directory (this directory).
    *   Add your Google AI Studio API key to the `.env` file like this: `GEMINI_API_KEY=YOUR_API_KEY_HERE`
    *   See the `.env.example` file for the format.
    *   The `.env` file is automatically ignored by git via `.gitignore`.
3.  **Compile `llama-gemma3-cli.exe` (for Gemma3 Vision Node):**
    *   This node requires the `llama-gemma3-cli.exe` executable from the `llama.cpp` project.
    *   Follow the instructions at [https://github.com/ggml-org/llama.cpp/discussions/12348](https://github.com/ggml-org/llama.cpp/discussions/12348) to clone and prepare the `llama.cpp` repository.
    *   **For GPU acceleration (Recommended):** Compile `llama-gemma3-cli` with CUDA support enabled. Delete any existing `build` directory inside `llama.cpp`, then run CMake configuration with the CUDA flag:
        ```powershell
        # Inside the llama.cpp directory
        cmake -B build -DLLAMA_CUBLAS=ON
        cmake --build build --target llama-gemma3-cli --config Release
        ```
        (This requires the NVIDIA CUDA Toolkit to be installed correctly.)
    *   **For CPU-only:** Compile without the CUDA flag (this will be much slower):
        ```powershell
        # Inside the llama.cpp directory
        cmake -B build
        cmake --build build --target llama-gemma3-cli --config Release
        ```
    *   Ensure the compiled `llama-gemma3-cli.exe` file exists (likely in `llama.cpp\build\bin\Release`). The node defaults to this path (adjust `DEFAULT_LLAMA_CLI_PATH` in the script or use the `cli_path_override` input if yours is different).
4.  Install/update the required Python dependencies:
    ```bash
    cd divergent_nodes
    pip install -r requirements.txt
    ```
5.  Restart ComfyUI.

The nodes should now be available in their respective categories when you right-click on the ComfyUI canvas.

## Nodes

### CLIP Token Counter

Counts the number of tokens generated by a CLIP tokenizer for the input text.

**Inputs:**

*   `text` (STRING): The text string you want to analyze.
*   `tokenizer_name` (STRING): The name of the Hugging Face CLIP tokenizer model to use (e.g., `openai/clip-vit-base-patch32`, `stabilityai/stable-diffusion-clip-vit-large-patch14`). Defaults to `openai/clip-vit-base-patch32`.

**Outputs:**

*   `token_count` (INT): The total number of tokens generated for the input text by the selected tokenizer.

**Category:** `Divergent Nodes 游놓/Text`

### Gemini API Node

Connects to the Google Gemini API to generate text based on a prompt and optional image input.

**Inputs:**

*   `model` (COMBO): Select the Gemini model to use. The list is dynamically fetched from the API if your API key is configured, otherwise defaults are shown.
*   `prompt` (STRING): The text prompt for the model.
*   `image_optional` (IMAGE): An optional image input for multimodal prompts.
*   `temperature` (FLOAT): Controls randomness (0.0-1.0). Lower values are more deterministic.
*   `top_p` (FLOAT): Nucleus sampling parameter (0.0-1.0).
*   `top_k` (INT): Top-k sampling parameter.
*   `max_output_tokens` (INT): Maximum number of tokens to generate.
*   `safety_harassment` (COMBO): Block threshold for harassment content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").
*   `safety_hate_speech` (COMBO): Block threshold for hate speech content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").
*   `safety_sexually_explicit` (COMBO): Block threshold for sexually explicit content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").
*   `safety_dangerous_content` (COMBO): Block threshold for dangerous content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").

**Outputs:**

*   `text` (STRING): The generated text response from the Gemini API.

**Category:** `Divergent Nodes 游놓/Gemini`

### Gemma3 Vision Node

Runs the Gemma 3 vision model using the experimental `llama-gemma3-cli` executable from `llama.cpp`. This allows for text generation based on prompts and optional image analysis.

**Prerequisites:**

*   You **must** compile `llama-gemma3-cli.exe` separately, ideally with CUDA support (see step 3 in Installation).
*   The node needs the correct path to this executable (either the default path hardcoded in the script or provided via `cli_path_override`).
*   The required models for the selected size (12B or 27B) will be downloaded automatically via `huggingface-hub` on first run to your Hugging Face cache.

**Inputs:**

*   `model_size` (COMBO): Select the model size ("12B" or "27B"). Defaults to "12B". This determines which models are downloaded and used.
*   `prompt` (STRING): The text prompt for the model.
*   `n_gpu_layers` (INT): Number of model layers to offload to the GPU. Set to 0 for CPU only, or a high number (e.g., 99) to offload as many as possible (requires CUDA-enabled build). Defaults to 99.
*   `image_optional` (IMAGE): An optional image input. If provided, the node saves it as a temporary PNG file and passes the path to the `llama-gemma3-cli` process.
*   `temperature` (FLOAT): Controls randomness (0.0-2.0). Defaults to 0.8.
*   `top_k` (INT): Top-k sampling parameter. Defaults to 40.
*   `top_p` (FLOAT): Nucleus sampling parameter (0.0-1.0). Defaults to 0.95.
*   `cli_path_override` (STRING): Optional. Provide the full path to your `llama-gemma3-cli.exe` if it's not in the default location specified in the script.

**Outputs:**

*   `text` (STRING): The generated text response from the Gemma 3 model via the CLI. Errors during execution will also be returned in this string.

**Category:** `Divergent Nodes 游놓/Gemma`

### KoboldCpp Node

Runs models using the KoboldCpp executable (`koboldcpp_cu12.exe` or similar). This node uses the `--prompt` flag for non-interactive text generation based on a prompt and various KoboldCpp settings.

**Prerequisites:**

*   You **must** have a working KoboldCpp executable (e.g., `koboldcpp_cu12.exe`). Download it from the [KoboldCpp releases page](https://github.com/LostRuins/koboldcpp/releases/latest).
*   The node needs the correct path to this executable (either the default path hardcoded in the script or provided via `koboldcpp_path`).
*   You need the `.gguf` model file(s) you intend to use.

**Important Note on Image Input:** This node currently uses KoboldCpp's `--prompt` mode, which does **not** support direct image file input via command-line arguments in the same way the interactive mode or API does. Providing an image to the `image_optional` input will result in an error message from the node. Text generation using multimodal models (with `--mmproj`) is supported, but the *input* must be text-only for now.

**Inputs:**

*   `koboldcpp_path` (STRING): Full path to your `koboldcpp_cu12.exe` (or equivalent). Defaults to a common location but should be verified.
*   `model_path` (STRING): Full path to the primary `.gguf` model file.
*   `prompt` (STRING): The text prompt for the model.
*   `gpu_acceleration` (COMBO): Select the GPU backend ("None", "CuBLAS", "CLBlast", "Vulkan"). Defaults to "CuBLAS". Note: CLBlast defaults to platform/device 0 0; use `extra_cli_args` for others.
*   `n_gpu_layers` (INT): Number of model layers to offload to the GPU (-1 for auto, 0 for CPU only). Defaults to -1.
*   `context_size` (INT): Maximum context size for the model. Defaults to 4096.
*   `max_output_tokens` (INT): Maximum number of tokens to generate (maps to `--promptlimit`). Defaults to 512.
*   `temperature` (FLOAT): Controls randomness. Defaults to 0.7.
*   `top_p` (FLOAT): Nucleus sampling parameter. Defaults to 0.92.
*   `top_k` (INT): Top-k sampling parameter (0 to disable). Defaults to 0.
*   `mmproj_path` (STRING): Optional. Full path to the multimodal projector (`.gguf`) file if using a vision model.
*   `image_optional` (IMAGE): Optional image input. **Currently non-functional** (see note above).
*   `threads` (INT): Number of CPU threads to use (0 for auto). Defaults to 0.
*   `use_mmap` (BOOLEAN): Enable memory-mapped file loading. Defaults to True.
*   `use_mlock` (BOOLEAN): Enable locking model in RAM. Defaults to False.
*   `flash_attention` (BOOLEAN): Enable Flash Attention (requires compatible GPU/backend, usually CuBLAS). Defaults to False.
*   `quant_kv` (COMBO): KV cache quantization level ("0: f16", "1: q8", "2: q4"). Defaults to "0: f16". Often requires Flash Attention.
*   `extra_cli_args` (STRING): Optional. Add any other valid KoboldCpp command-line flags here (e.g., `--useclblast 1 0`, `--nommq`).

**Outputs:**

*   `text` (STRING): The generated text response from KoboldCpp. Errors during execution will also be returned in this string.

**Category:** `Divergent Nodes 游놓/KoboldCpp`
