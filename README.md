# Divergent Nodes - Custom ComfyUI Nodes

This repository contains custom nodes for ComfyUI.

## Installation

1.  Clone this repository into your `ComfyUI/custom_nodes/` directory:
    ```bash
    cd ComfyUI/custom_nodes/
    git clone https://github.com/thedivergentai/divergent_nodes.git divergent_nodes
    ```
2.  **Set up API Key (for Gemini Node):**
    *   Create a `.env` file in the `divergent_nodes` directory (this directory).
    *   Add your Google AI Studio API key to the `.env` file like this: `GEMINI_API_KEY=YOUR_API_KEY_HERE`
    *   See the `.env.example` file for the format.
    *   The `.env` file is automatically ignored by git via `.gitignore`.
3.  **Compile `llama-gemma3-cli.exe` (for Gemma3 Vision Node):**
    *   This node requires the `llama-gemma3-cli.exe` executable from the `llama.cpp` project.
    *   Follow the instructions at [https://github.com/ggml-org/llama.cpp/discussions/12348](https://github.com/ggml-org/llama.cpp/discussions/12348) to compile `llama.cpp` and specifically the `llama-gemma3-cli` target using CMake.
    *   Ensure the compiled `.exe` file exists. The node defaults to looking for it at `C:\Users\YOUR_USERNAME\Documents\llama_cpp_build\llama.cpp\build\bin\Debug\llama-gemma3-cli.exe` (adjust the path in `gemma3_vision_node.py` or use the `cli_path_override` input if yours is different).
4.  Install/update the required Python dependencies:
    ```bash
    cd divergent_nodes
    pip install -r requirements.txt
    ```
5.  Restart ComfyUI.

The nodes should now be available in their respective categories when you right-click on the ComfyUI canvas.

## Nodes

### CLIP Token Counter

Counts the number of tokens generated by a CLIP tokenizer for the input text.

**Inputs:**

*   `text` (STRING): The text string you want to analyze.
*   `tokenizer_name` (STRING): The name of the Hugging Face CLIP tokenizer model to use (e.g., `openai/clip-vit-base-patch32`, `stabilityai/stable-diffusion-clip-vit-large-patch14`). Defaults to `openai/clip-vit-base-patch32`.

**Outputs:**

*   `token_count` (INT): The total number of tokens generated for the input text by the selected tokenizer.

**Category:** `Divergent Nodes ðŸ‘½/Text`

### Gemini API Node

Connects to the Google Gemini API to generate text based on a prompt and optional image input.

**Inputs:**

*   `model` (COMBO): Select the Gemini model to use. The list is dynamically fetched from the API if your API key is configured, otherwise defaults are shown.
*   `prompt` (STRING): The text prompt for the model.
*   `image_optional` (IMAGE): An optional image input for multimodal prompts.
*   `temperature` (FLOAT): Controls randomness (0.0-1.0). Lower values are more deterministic.
*   `top_p` (FLOAT): Nucleus sampling parameter (0.0-1.0).
*   `top_k` (INT): Top-k sampling parameter.
*   `max_output_tokens` (INT): Maximum number of tokens to generate.
*   `safety_harassment` (COMBO): Block threshold for harassment content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").
*   `safety_hate_speech` (COMBO): Block threshold for hate speech content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").
*   `safety_sexually_explicit` (COMBO): Block threshold for sexually explicit content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").
*   `safety_dangerous_content` (COMBO): Block threshold for dangerous content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").

**Outputs:**

*   `text` (STRING): The generated text response from the Gemini API.

**Category:** `Divergent Nodes ðŸ‘½/Gemini`

### Gemma3 Vision Node

Runs the Gemma 3 vision model using the experimental `llama-gemma3-cli` executable from `llama.cpp`. This allows for text generation based on prompts and optional image analysis.

**Prerequisites:**

*   You **must** compile `llama-gemma3-cli.exe` separately (see step 3 in Installation).
*   The node needs the correct path to this executable (either the default path hardcoded in the script or provided via `cli_path_override`).
*   The required models (`gemma-3-27b-it-abliterated-Q4_K_M.gguf` and `mmproj-gemma-3-27b-it-abliterated-f32.gguf`) will be downloaded automatically via `huggingface-hub` on first run to your Hugging Face cache.

**Inputs:**

*   `prompt` (STRING): The text prompt for the model.
*   `image_optional` (IMAGE): An optional image input. If provided, the node saves it as a temporary PNG file and passes the path to the `llama-gemma3-cli` process.
*   `temperature` (FLOAT): Controls randomness (0.0-2.0). Defaults to 0.8.
*   `top_k` (INT): Top-k sampling parameter. Defaults to 40.
*   `top_p` (FLOAT): Nucleus sampling parameter (0.0-1.0). Defaults to 0.95.
*   `cli_path_override` (STRING): Optional. Provide the full path to your `llama-gemma3-cli.exe` if it's not in the default location specified in the script.

**Outputs:**

*   `text` (STRING): The generated text response from the Gemma 3 model via the CLI. Errors during execution will also be returned in this string.

**Category:** `Divergent Nodes ðŸ‘½/Gemma`
