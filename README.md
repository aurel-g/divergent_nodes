# Divergent Nodes - Custom ComfyUI Nodes

This repository contains custom nodes for ComfyUI.

## Installation

1.  Clone this repository into your `ComfyUI/custom_nodes/` directory:
    ```bash
    cd ComfyUI/custom_nodes/
    git clone https://github.com/thedivergentai/divergent_nodes.git divergent_nodes
    ```
2.  **Set up API Key (for Gemini Node):**
    *   Create a `.env` file in the `divergent_nodes` directory (this directory).
    *   Add your Google AI Studio API key to the `.env` file like this: `GEMINI_API_KEY=YOUR_API_KEY_HERE`
    *   See the `.env.example` file for the format.
    *   The `.env` file is automatically ignored by git via `.gitignore`.
3.  Install/update the required Python dependencies:
    ```bash
    cd divergent_nodes
    pip install -r requirements.txt
    ```
4.  Restart ComfyUI.

The nodes should now be available in their respective categories when you right-click on the ComfyUI canvas.

## Nodes

### CLIP Token Counter

Counts the number of tokens generated by a CLIP tokenizer for the input text.

**Inputs:**

*   `text` (STRING): The text string you want to analyze.
*   `tokenizer_name` (STRING): The name of the Hugging Face CLIP tokenizer model to use (e.g., `openai/clip-vit-base-patch32`, `stabilityai/stable-diffusion-clip-vit-large-patch14`). Defaults to `openai/clip-vit-base-patch32`.

**Outputs:**

*   `token_count` (INT): The total number of tokens generated for the input text by the selected tokenizer.

**Category:** `Divergent Nodes ðŸ‘½/Text`

### Gemini API Node

Connects to the Google Gemini API to generate text based on a prompt and optional image input.

**Inputs:**

*   `model` (COMBO): Select the Gemini model to use. The list is dynamically fetched from the API if your API key is configured, otherwise defaults are shown.
*   `prompt` (STRING): The text prompt for the model.
*   `image_optional` (IMAGE): An optional image input for multimodal prompts.
*   `temperature` (FLOAT): Controls randomness (0.0-1.0). Lower values are more deterministic.
*   `top_p` (FLOAT): Nucleus sampling parameter (0.0-1.0).
*   `top_k` (INT): Top-k sampling parameter.
*   `max_output_tokens` (INT): Maximum number of tokens to generate.
*   `safety_harassment` (COMBO): Block threshold for harassment content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").
*   `safety_hate_speech` (COMBO): Block threshold for hate speech content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").
*   `safety_sexually_explicit` (COMBO): Block threshold for sexually explicit content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").
*   `safety_dangerous_content` (COMBO): Block threshold for dangerous content (Options: "Default (Unspecified)", "Block Low & Above", "Block Medium & Above", "Block High Only", "Block None").

**Outputs:**

*   `text` (STRING): The generated text response from the Gemini API.

**Category:** `Divergent Nodes ðŸ‘½/Gemini`

### KoboldCpp Node

Runs models using the KoboldCpp executable (`koboldcpp_cu12.exe` or similar). This node launches KoboldCpp with setup parameters via the command line and passes generation parameters (including prompt and optional image data) via a JSON payload on standard input.

**Prerequisites:**

*   You **must** have a working KoboldCpp executable (e.g., `koboldcpp_cu12.exe`). Download it from the [KoboldCpp releases page](https://github.com/LostRuins/koboldcpp/releases/latest).
*   The node needs the correct path to this executable (either the default path hardcoded in the script or provided via `koboldcpp_path`).
*   You need the `.gguf` model file(s) you intend to use.
*   For image input, you need the corresponding multimodal projector (`.gguf`) file specified in `mmproj_path`.

**How it Works:**
The node starts the KoboldCpp process with configuration flags (like model path, GPU layers, context size). It then sends a JSON object containing the prompt, generation settings (temperature, max length, etc.), and optionally a Base64 encoded image to the process's standard input. KoboldCpp processes this input and writes the generated text to its standard output, which the node captures and returns.

**Inputs:**

*   **Setup Arguments (Passed via CLI):**
    *   `koboldcpp_path` (STRING): Full path to your `koboldcpp_cu12.exe` (or equivalent). Defaults to a common location but should be verified.
    *   `model_path` (STRING): Full path to the primary `.gguf` model file.
    *   `gpu_acceleration` (COMBO): Select the GPU backend ("None", "CuBLAS", "CLBlast", "Vulkan"). Defaults to "CuBLAS". Note: CLBlast defaults to platform/device 0 0; use `extra_cli_args` for others.
    *   `n_gpu_layers` (INT): Number of model layers to offload to the GPU (-1 for auto, 0 for CPU only). Defaults to -1.
    *   `context_size` (INT): Maximum context size for the model. Defaults to 4096.
    *   `mmproj_path` (STRING): Optional. Full path to the multimodal projector (`.gguf`) file if using a vision model. **Required for image input.**
    *   `threads` (INT): Number of CPU threads to use (0 for auto). Defaults to 0.
    *   `use_mmap` (BOOLEAN): Enable memory-mapped file loading. Defaults to True.
    *   `use_mlock` (BOOLEAN): Enable locking model in RAM. Defaults to False.
    *   `flash_attention` (BOOLEAN): Enable Flash Attention (requires compatible GPU/backend, usually CuBLAS). Defaults to False.
    *   `quant_kv` (COMBO): KV cache quantization level ("0: f16", "1: q8", "2: q4"). Defaults to "0: f16". Often requires Flash Attention.
    *   `extra_cli_args` (STRING): Optional. Add any other valid KoboldCpp *setup* command-line flags here (e.g., `--useclblast 1 0`, `--nommq`). **Do not** include generation flags like `--temp` or `--prompt` here.
*   **Generation Arguments (Passed via JSON on stdin):**
    *   `prompt` (STRING): The text prompt for the model. If an image is provided, the node automatically formats this like `\n(Attached Image)\n\n### Instruction:\n{your_prompt}\n### Response:\n`.
    *   `max_length` (INT): Maximum number of tokens to generate. Defaults to 512.
    *   `temperature` (FLOAT): Controls randomness. Defaults to 0.7.
    *   `top_p` (FLOAT): Nucleus sampling parameter. Defaults to 0.92.
    *   `top_k` (INT): Top-k sampling parameter (0 to disable). Defaults to 0.
    *   `rep_pen` (FLOAT): Repetition penalty. Defaults to 1.1.
    *   `image_optional` (IMAGE): Optional image input. If provided, it's converted to Base64 and sent in the JSON payload. Requires `mmproj_path` to be set correctly.
    *   `stop_sequence` (STRING): Optional. Comma or newline-separated list of sequences to stop generation at.

**Outputs:**

*   `text` (STRING): The generated text response from KoboldCpp. Errors during execution will also be returned in this string.

**Category:** `Divergent Nodes ðŸ‘½/KoboldCpp`
